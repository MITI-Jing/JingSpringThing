{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e5c455d",
   "metadata": {},
   "source": [
    "updated code to capture dates from all possible elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7d7d129",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL</th>\n",
       "      <th>Published Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://[[ComfyWorkFlows]].com/</td>\n",
       "      <td>Error: https://[[ComfyWorkFlows]].com/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://[[ComfyWorkFlows]].com/</td>\n",
       "      <td>Error: https://[[ComfyWorkFlows]].com/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://miro.com/app/board/uXjVPzJyAtU=/</td>\n",
       "      <td>No published date found</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://openai.com/blog/chatgpt</td>\n",
       "      <td>No published date found</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://customgpt.ai</td>\n",
       "      <td>2023-06-09T14:28:29-04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2826</th>\n",
       "      <td>https://arxiv.org/pdf/2102.12092.pdf</td>\n",
       "      <td>Error: 'utf-8' codec can't decode byte 0x8f in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2827</th>\n",
       "      <td>https://arxiv.org/pdf/2309.17444.pdf</td>\n",
       "      <td>Error: 'utf-8' codec can't decode byte 0x8f in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2828</th>\n",
       "      <td>https://advisor.morganstanley.com/daron.edward...</td>\n",
       "      <td>Error:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2829</th>\n",
       "      <td>https://www.minecraft.net/en-us/article/minecr...</td>\n",
       "      <td>Error:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2830</th>\n",
       "      <td>https://www.nasdaq.com/articles/a-comparison-o...</td>\n",
       "      <td>Error:</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2831 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    URL  \\\n",
       "0                       https://[[ComfyWorkFlows]].com/   \n",
       "1                       https://[[ComfyWorkFlows]].com/   \n",
       "2              https://miro.com/app/board/uXjVPzJyAtU=/   \n",
       "3                       https://openai.com/blog/chatgpt   \n",
       "4                                  https://customgpt.ai   \n",
       "...                                                 ...   \n",
       "2826               https://arxiv.org/pdf/2102.12092.pdf   \n",
       "2827               https://arxiv.org/pdf/2309.17444.pdf   \n",
       "2828  https://advisor.morganstanley.com/daron.edward...   \n",
       "2829  https://www.minecraft.net/en-us/article/minecr...   \n",
       "2830  https://www.nasdaq.com/articles/a-comparison-o...   \n",
       "\n",
       "                                         Published Date  \n",
       "0                Error: https://[[ComfyWorkFlows]].com/  \n",
       "1                Error: https://[[ComfyWorkFlows]].com/  \n",
       "2                               No published date found  \n",
       "3                               No published date found  \n",
       "4                             2023-06-09T14:28:29-04:00  \n",
       "...                                                 ...  \n",
       "2826  Error: 'utf-8' codec can't decode byte 0x8f in...  \n",
       "2827  Error: 'utf-8' codec can't decode byte 0x8f in...  \n",
       "2828                                            Error:   \n",
       "2829                                            Error:   \n",
       "2830                                            Error:   \n",
       "\n",
       "[2831 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Published dates have been saved to C:\\Users\\lolic\\OneDrive\\Desktop\\work stuff\\MediaCity Immersive Innovation Hub(Dreamlab)\\WebScraping\\url_dates.csv\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import aiohttp\n",
    "import asyncio\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from dateutil.parser import parse, UnknownTimezoneWarning\n",
    "from aiohttp import ClientSession\n",
    "import nest_asyncio\n",
    "import warnings\n",
    "from IPython.display import display\n",
    "\n",
    "# Apply nest_asyncio to allow nested event loops in Jupyter notebooks\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Suppress specific warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='bs4')\n",
    "warnings.filterwarnings(\"ignore\", category=UnknownTimezoneWarning)\n",
    "\n",
    "def extract_dates_from_html(html_content):\n",
    "    date_patterns = [\n",
    "        r'\\b\\d{1,2}[-/]\\d{1,2}[-/]\\d{2,4}\\b',  # Matches dates like 12/31/2020, 12-31-2020\n",
    "        r'\\b\\d{4}[-/]\\d{1,2}[-/]\\d{1,2}\\b',    # Matches dates like 2020/12/31, 2020-12-31\n",
    "        r'\\b\\d{1,2} \\w+ \\d{4}\\b',              # Matches dates like 31 December 2020\n",
    "        r'\\b\\w+ \\d{1,2}, \\d{4}\\b',             # Matches dates like December 31, 2020\n",
    "    ]\n",
    "    dates = []\n",
    "    for pattern in date_patterns:\n",
    "        matches = re.findall(pattern, html_content)\n",
    "        for match in matches:\n",
    "            try:\n",
    "                parsed_date = parse(match, fuzzy=True, tzinfos={\"EST\": -18000})\n",
    "                dates.append(parsed_date.isoformat())\n",
    "            except ValueError:\n",
    "                continue\n",
    "    return dates\n",
    "\n",
    "def extract_published_date(soup, url):\n",
    "    # Special handling for GitHub URLs\n",
    "    if 'github.com' in url:\n",
    "        return extract_github_date(url)\n",
    "    \n",
    "    # General meta tags and common class names\n",
    "    date_patterns = [\n",
    "        {'name': 'meta', 'attr': 'property', 'value': 'article:published_time'},\n",
    "        {'name': 'meta', 'attr': 'property', 'value': 'og:published_time'},\n",
    "        {'name': 'meta', 'attr': 'name', 'value': 'date'},\n",
    "        {'name': 'meta', 'attr': 'itemprop', 'value': 'datePublished'},\n",
    "        {'name': 'meta', 'attr': 'property', 'value': 'datePublished'},\n",
    "        {'name': 'meta', 'attr': 'name', 'value': 'publish_date'},\n",
    "        {'name': 'meta', 'attr': 'name', 'value': 'PubDate'},\n",
    "    ]\n",
    "    \n",
    "    for pattern in date_patterns:\n",
    "        tag = soup.find(pattern['name'], {pattern['attr']: pattern['value']})\n",
    "        if tag and tag.get('content'):\n",
    "            try:\n",
    "                return parse(tag['content'], fuzzy=True, tzinfos={\"EST\": -18000}).isoformat()\n",
    "            except ValueError:\n",
    "                continue\n",
    "    \n",
    "    # Check for common class names and text locations\n",
    "    date_classes = ['published-date', 'pub-date', 'date', 'publish-date', 'date-posted', 'posted-date']\n",
    "    for class_name in date_classes:\n",
    "        tag = soup.find(class_=class_name)\n",
    "        if tag:\n",
    "            try:\n",
    "                return parse(tag.get_text(), fuzzy=True, tzinfos={\"EST\": -18000}).isoformat()\n",
    "            except ValueError:\n",
    "                continue\n",
    "    \n",
    "    # Check for dates near title elements\n",
    "    title_tag = soup.find(['h1', 'h2', 'h3'])\n",
    "    if title_tag:\n",
    "        sibling_text = ' '.join(sibling.get_text() for sibling in title_tag.next_siblings if sibling.name in ['p', 'div', 'span'])\n",
    "        dates = extract_dates_from_html(sibling_text)\n",
    "        if dates:\n",
    "            return dates[0]\n",
    "\n",
    "    # Check entire text of the article as last resort\n",
    "    text_content = soup.get_text()\n",
    "    dates = extract_dates_from_html(text_content)\n",
    "    return dates[0] if dates else 'No published date found'\n",
    "\n",
    "async def extract_github_date(url):\n",
    "    api_url = url.replace(\"https://github.com/\", \"https://api.github.com/repos/\")\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        async with session.get(api_url) as response:\n",
    "            data = await response.json()\n",
    "            if 'created_at' in data:\n",
    "                return parse(data['created_at']).isoformat()\n",
    "            return 'No date found'\n",
    "\n",
    "async def fetch(session, url):\n",
    "    try:\n",
    "        async with session.get(url) as response:\n",
    "            html = await response.text()\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "            date = extract_published_date(soup, url)\n",
    "            return url, date\n",
    "    except Exception as e:\n",
    "        return url, f'Error: {str(e)}'\n",
    "\n",
    "async def process_urls_async(urls):\n",
    "    url_dates = []\n",
    "    async with ClientSession() as session:\n",
    "        tasks = [fetch(session, url) for url in urls]\n",
    "        for task in asyncio.as_completed(tasks):\n",
    "            url, date = await task\n",
    "            url_dates.append({'URL': url, 'Published Date': date})\n",
    "    return url_dates\n",
    "\n",
    "# Example usage:\n",
    "csv_file_path = r'C:\\Users\\lolic\\OneDrive\\Desktop\\work stuff\\MediaCity Immersive Innovation Hub(Dreamlab)\\WebScraping\\extracted_urls.csv'  # Path to your CSV file with URLs\n",
    "df = pd.read_csv(csv_file_path)\n",
    "urls = df['URL'].tolist()\n",
    "\n",
    "# Process URLs asynchronously\n",
    "url_dates = asyncio.run(process_urls_async(urls))\n",
    "\n",
    "# Convert the results to a DataFrame for better visualization\n",
    "df_dates = pd.DataFrame(url_dates)\n",
    "\n",
    "# Save to a new CSV file\n",
    "output_file_path = r'C:\\Users\\lolic\\OneDrive\\Desktop\\work stuff\\MediaCity Immersive Innovation Hub(Dreamlab)\\WebScraping\\url_dates.csv'  # Desired output path\n",
    "df_dates.to_csv(output_file_path, index=False)\n",
    "\n",
    "# Display the DataFrame\n",
    "display(df_dates)\n",
    "print(f\"Published dates have been saved to {output_file_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
