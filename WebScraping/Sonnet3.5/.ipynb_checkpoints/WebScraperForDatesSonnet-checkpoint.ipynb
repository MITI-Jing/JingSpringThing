{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11eb3ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "import concurrent.futures\n",
    "from bs4 import BeautifulSoup, MarkupResemblesLocatorWarning\n",
    "from datetime import datetime\n",
    "import dateutil.parser\n",
    "import json\n",
    "import warnings\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from dateutil.tz import gettz\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=MarkupResemblesLocatorWarning)\n",
    "\n",
    "# Mapping for unknown timezones\n",
    "tzinfos = {\n",
    "    'EDT': gettz('America/New_York'),\n",
    "    'EST': gettz('America/New_York'),\n",
    "    'PST': gettz('America/Los_Angeles'),\n",
    "    'PDT': gettz('America/Los_Angeles'),\n",
    "    'IST': gettz('Asia/Kolkata'),\n",
    "    'PT': gettz('America/Los_Angeles')\n",
    "}\n",
    "\n",
    "def parse_date(date_str):\n",
    "    try:\n",
    "        return dateutil.parser.parse(date_str, fuzzy=True, tzinfos=tzinfos).strftime('%Y-%m-%d')\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def extract_date(url, html_content, content_type):\n",
    "    if 'xml' in content_type:\n",
    "        soup = BeautifulSoup(html_content, 'xml')\n",
    "    else:\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    potential_dates = []\n",
    "\n",
    "    # 1. Check meta tags\n",
    "    meta_tags = soup.find_all('meta')\n",
    "    for tag in meta_tags:\n",
    "        if tag.get('property') in ['article:published_time', 'og:published_time', 'pubdate', 'datePublished', 'date']:\n",
    "            potential_dates.append(tag.get('content'))\n",
    "\n",
    "    # 2. Check LD+JSON\n",
    "    ld_json = soup.find_all('script', type='application/ld+json')\n",
    "    for script in ld_json:\n",
    "        try:\n",
    "            data = json.loads(script.string)\n",
    "            if isinstance(data, list):\n",
    "                data = data[0]\n",
    "            date = data.get('datePublished') or data.get('dateCreated') or data.get('dateModified')\n",
    "            if date:\n",
    "                potential_dates.append(date)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    # 3. Check time tags\n",
    "    time_tags = soup.find_all('time')\n",
    "    for tag in time_tags:\n",
    "        if tag.has_attr('datetime'):\n",
    "            potential_dates.append(tag['datetime'])\n",
    "        elif tag.string:\n",
    "            potential_dates.append(tag.string)\n",
    "\n",
    "    # 4. Look for date patterns in text\n",
    "    text = soup.get_text()\n",
    "    date_patterns = [\n",
    "        r'\\d{4}-\\d{2}-\\d{2}',\n",
    "        r'\\d{2}/\\d{2}/\\d{4}',\n",
    "        r'\\d{2}-\\d{2}-\\d{4}',\n",
    "        r'\\b(?:January|February|March|April|May|June|July|August|September|October|November|December)\\s+\\d{1,2},?\\s+\\d{4}\\b',\n",
    "        r'\\b(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\\s+\\d{1,2},?\\s+\\d{4}\\b'\n",
    "    ]\n",
    "    for pattern in date_patterns:\n",
    "        matches = re.findall(pattern, text)\n",
    "        potential_dates.extend(matches)\n",
    "\n",
    "    # 5. Check URL for date\n",
    "    url_date_patterns = [\n",
    "        r'/(\\d{4}/\\d{2}/\\d{2})/',\n",
    "        r'(\\d{4}-\\d{2}-\\d{2})',\n",
    "        r'(\\d{2}-\\d{2}-\\d{4})',\n",
    "    ]\n",
    "    for pattern in url_date_patterns:\n",
    "        match = re.search(pattern, url)\n",
    "        if match:\n",
    "            potential_dates.append(match.group(1))\n",
    "\n",
    "    # Parse and validate all potential dates\n",
    "    valid_dates = [parse_date(date) for date in potential_dates if parse_date(date)]\n",
    "    \n",
    "    if valid_dates:\n",
    "        # Return the oldest date found\n",
    "        return min(valid_dates)\n",
    "    \n",
    "    return None\n",
    "\n",
    "def analyze_url(url):\n",
    "    try:\n",
    "        parsed = urlparse(url)\n",
    "        domain = parsed.netloc.lower()\n",
    "\n",
    "        known_domains = {\n",
    "            'twitter.com': 'Twitter',\n",
    "            'x.com': 'Twitter',\n",
    "            'github.com': 'GitHub',\n",
    "            'youtube.com': 'YouTube',\n",
    "            'youtu.be': 'YouTube',\n",
    "            'linkedin.com': 'LinkedIn',\n",
    "            'facebook.com': 'Facebook',\n",
    "            'fb.com': 'Facebook',\n",
    "            'instagram.com': 'Instagram',\n",
    "            'reddit.com': 'Reddit',\n",
    "            'medium.com': 'Medium',\n",
    "            'stackoverflow.com': 'Stack Overflow',\n",
    "            'wikipedia.org': 'Wikipedia',\n",
    "            'amazon.com': 'Amazon',\n",
    "            'dropbox.com': 'Dropbox',\n",
    "        }\n",
    "        \n",
    "        for known_domain, site_type in known_domains.items():\n",
    "            if known_domain in domain:\n",
    "                return site_type, None\n",
    "\n",
    "        # Setting up a retry strategy\n",
    "        session = requests.Session()\n",
    "        retries = Retry(total=5, backoff_factor=0.1, status_forcelist=[500, 502, 503, 504])\n",
    "        session.mount('http://', HTTPAdapter(max_retries=retries))\n",
    "        session.mount('https://', HTTPAdapter(max_retries=retries))\n",
    "\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}\n",
    "        response = session.get(url, headers=headers, timeout=10)\n",
    "        content_type = response.headers.get('Content-Type', '').lower()\n",
    "        \n",
    "        published_date = extract_date(url, response.text, content_type)\n",
    "\n",
    "        if 'text/html' in content_type or 'application/xhtml+xml' in content_type:\n",
    "            return 'Web Page', published_date\n",
    "        elif 'application/pdf' in content_type:\n",
    "            return 'PDF', published_date\n",
    "        elif any(img_type in content_type for img_type in ['image/jpeg', 'image/png', 'image/gif']):\n",
    "            return 'Image', published_date\n",
    "        elif 'application/json' in content_type:\n",
    "            return 'API', published_date\n",
    "        elif any(doc_type in content_type for doc_type in ['msword', 'vnd.openxmlformats-officedocument', 'vnd.ms-excel']):\n",
    "            return 'Document', published_date\n",
    "        else:\n",
    "            return 'Unknown', published_date\n",
    "    except Exception as e:\n",
    "        return f'Error: {str(e)}', None\n",
    "\n",
    "def process_url(row):\n",
    "    filename, url = row\n",
    "    url_type, published_date = analyze_url(url)\n",
    "    return [filename, url, url_type, published_date]\n",
    "\n",
    "def update_csv_with_types_and_dates(input_file, output_file):\n",
    "    with open(input_file, 'r', newline='', encoding='utf-8') as infile, \\\n",
    "         open(output_file, 'w', newline='', encoding='utf-8') as outfile:\n",
    "        reader = csv.reader(infile)\n",
    "        writer = csv.writer(outfile)\n",
    "        \n",
    "        header = next(reader)\n",
    "        header.extend(['URL Type', 'Published Date'])\n",
    "        writer.writerow(header)\n",
    "        \n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "            results = list(executor.map(process_url, reader))\n",
    "        \n",
    "        writer.writerows(results)\n",
    "\n",
    "    print(f\"Updated CSV file saved as {output_file}\")\n",
    "\n",
    "# File paths\n",
    "input_csv = r'C:\\Users\\lolic\\OneDrive\\Desktop\\work stuff\\MediaCity Immersive Innovation Hub(Dreamlab)\\WebScraping\\Sonnet3.5\\extracted_urls.csv'\n",
    "output_csv = r'C:\\Users\\lolic\\OneDrive\\Desktop\\work stuff\\MediaCity Immersive Innovation Hub(Dreamlab)\\WebScraping\\Sonnet3.5\\extracted_urls_with_types_and_dates.csv'\n",
    "\n",
    "update_csv_with_types_and_dates(input_csv, output_csv)\n",
    "\n",
    "url_type_counts = {}\n",
    "date_count = 0\n",
    "with open(output_csv, 'r', newline='', encoding='utf-8') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    next(reader)  # Skip header\n",
    "    for row in reader:\n",
    "        url_type = row[2]\n",
    "        url_type_counts[url_type] = url_type_counts.get(url_type, 0) + 1\n",
    "        if row[3]:  # Count non-empty dates\n",
    "            date_count += 1\n",
    "\n",
    "print(\"\\nURL Type Summary:\")\n",
    "for url_type, count in url_type_counts.items():\n",
    "    print(f\"{url_type}: {count}\")\n",
    "\n",
    "print(f\"\\nTotal URLs with extracted dates: {date_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c242bda5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
